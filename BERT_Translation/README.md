**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 中文翻译**

本资源完整的翻译了论文，并且给出了论文中所有引用资料的网络连接，方便对 BERT 感兴趣的朋友们进一步研究 BERT。

1. 原文 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v1)，这是BERT在2018年11月发布的版本，与2019年5月版本[v2](https://arxiv.org/abs/1810.04805v2)有稍许不同。
3. BERT论文翻译 [PDF版下载](BERT中文翻译PDF版.pdf) 。
5. 原文由袁宵翻译，地址见：https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation 。

