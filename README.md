# NLP_Paper_Translation
nlp相关论文中文翻译，从NNLM到XLNET等。

主要目录结构是按照NLP的发展历程，从NNLM到XLNET的论文翻译。部分翻译由网友提供，因为已经翻译好了，就不在重复翻译。

**说明**：

1.本资源完整的翻译了论文，并且给出了论文中所有引用资料的网络连接，方便对 NLP 感兴趣的朋友们进一步研究。

2.所有的翻译均提供PDF文档下载，详细下载见各论文的目录。

3.个人时间有限，目前先翻译最新的论文。

4.转载请注明出处，商用请联系译者 徐东：dongxu222mk@163.com

5.如果你喜欢我的工作，请点亮右上角星星，谢谢 :smiley:

---

**主要目录**：

| NLP发展历程        | 论文                                                         | 翻译进展 |
| ------------------ | ------------------------------------------------------------ | -------- |
| 1.神经网络语言模型 | [A Neural Probabilistic Language Model](https://www.researchgate.net/publication/221618573_A_Neural_Probabilistic_Language_Model) |          |
| 2.word2vec         | 1.Efficient estimation of word representations in vector space<br />2.Distributed Representations of Words and Phrases and their Compositionality |          |
| 3.Glove            | *GloVe*:Global Vectors for Word Representation               |          |
| 4.fasttext         | 1.Enriching Word Vectors with Subword Information<br />2.Bag of Tricks for Efficient Text Classification |          |
| 5.PV-DM&PV-DBOW    | Distributed Representations of Sentences and Documents       |          |
| 6.Skip-Thought     | Skip-Thought Vectors                                         |          |
| 7.Quick-Thought    | An efficient framework for learning sentence representations |          |
| 8.InferSent        | Supervised Learning of Universal Sentence Representations from Natural Language Inference Data |          |
| 9.GPSE             | [Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning](https://openreview.net/pdf?id=B18WgG-CZ) |          |
| 10.USE             | [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf) |          |
| 11.Cove            | Learned in Translation: Contextualized Word Vectors          |          |
| 12.ELMO            | Deep contextualized word representations                     |          |
| 13.ULMFit          | Universal Language Model Fine-tuning for Text Classification |          |
| 14.GPT             | Improving Language Understanding by Generative Pre-Training  |          |
| 15.Transformer     | Attention is all you need                                    |          |
| 16.BERT            | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding |          |
| 17.Transformer-xl  | *Transformer-XL*: Attentive Language Models Beyond a Fixed-Length Context |          |
| 16.XLNET           | XLNet: Generalized Autoregressive Pretraining for Language Understanding |          |