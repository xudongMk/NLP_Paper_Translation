**Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context 中文翻译**

本资源完整的翻译了论文，并且给出了论文中所有引用资料的网络连接，方便对 Transformer-XL 感兴趣的朋友们进一步研究。

1. 原文 [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context ](https://arxiv.org/abs/1901.02860)

   论文总共有三个版本

   **[[v1]](https://arxiv.org/abs/1901.02860v1)** Wed, 9 Jan 2019 18:28:19 UTC (2,124 KB)
   
   **[[v2]](https://arxiv.org/abs/1901.02860v2)** Fri, 18 Jan 2019 18:38:00 UTC (2,126 KB)
   
   **[v3]** Sun, 2 Jun 2019 21:21:48 UTC (2,457 KB)

   本文翻译是第三个版本。

2. 论文代码：https://github.com/kimiyoung/transformer-xl

3. 论文翻译 [PDF版下载]。

4. 部分翻译来自：https://github.com/powerycy/NLP/blob/master/documents/%E6%A8%A1%E5%9E%8B/Transformer-XL.md 在此基础上进行了修改。

