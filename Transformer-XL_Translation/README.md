**Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context 中文翻译**

本资源完整的翻译了论文，并且给出了论文中所有引用资料的网络连接，方便对 Transformer-XL 感兴趣的朋友们进一步研究。

1. 原文 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1901.02860)

   论文总共有三个版本**[[v1\]](https://arxiv.org/abs/1901.02860v1)** Wed, 9 Jan 2019 18:28:19 UTC (2,124 KB)
   **[[v2\]](https://arxiv.org/abs/1901.02860v2)** Fri, 18 Jan 2019 18:38:00 UTC (2,126 KB)
   **[v3]** Sun, 2 Jun 2019 21:21:48 UTC (2,457 KB)

   本文翻译是第三个版本。

2. 论文代码：https://github.com/kimiyoung/transformer-xl

3. 论文翻译 [PDF版下载]。

