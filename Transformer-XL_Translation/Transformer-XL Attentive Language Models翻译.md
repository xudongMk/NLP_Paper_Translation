## Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

**Zihang Dai***∗*12**, Zhilin Yang***∗*12**, Yiming Yang**1 **, Jaime Carbonell**1**,**

**Quoc V. Le**2 **, Ruslan Salakhutdinov**1 

1Carnegie Mellon University, 2Google Brain

{dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com

### 摘要

Transformers 具有学习长期依赖的潜力，但在语言模型设置中受到固定长度的上下文的限制。我们提出了一种新的语言模型 Transformers-XL，它可以在不破坏时间一致性的情况下，使学习依赖关系超出固定长度的限制。它包括一种分段recurrence机制和一种新的位置编码方案。我们的方法不仅能够捕获长期依赖关系，而且还解决了上下文固定长度问题。Transformer-XL学习的依赖关系比RNNs长80%，比Transformers长450%，在短序列和长序列上都能获得更好的性能，在评估期间比普通的Transformers快1800多倍。值得注意的是，我们将enwiki8上的bpc/perplexity的最新结果达到了0.99，text8达到了1.08，WikiText-103达到了18.3，Billion Word达到21.8，Penn Treebank达到54.5（无微调）。当只在WikiText-103上进行训练时，Transformer XL设法生成具有数千个标记的合理连贯、新颖的文本文章。我们的代码、预训练模型和超参数可以同时在Tensorflow和PyTorch中【https://github.com/kimiyoung/transformer-xl】使用。

### 1 简介

语言模型是最重要的问题之一，需要建模长期依赖性，成功的应用如无监督预训练(Dai and Le, 2015;Peters 等，2018;Radford 等，2018;Devlin 等，2018)。然而，如何使神经网络具备对序列数据进行长期依赖建模的能力一直是一个挑战。递归神经网络(RNNs)，特别是长短期记忆(LSTM)网络(Hochreiter and Schmidhuber, 1997)已经成为语言建模的标准解决方案，在多个基准测试上取得了较好的结果。尽管RNNs具有广泛的适应性，但由于梯度消失和爆炸，RNNs很难优化(Hochreiter 等， 2001)，而在LSTMs中引入门控和梯度剪切技术(Graves, 2013)可能不足以完全解决这个问题。根据经验，先前的研究发现，LSTM语言模型平均使用200个上下文单词(Khandelwal et al.， 2018)，这表明还有进一步改进的空间。

另一方面，在注意机制中远距离单词对之间的直接连接可能会更易于优化，并能够学习长期依赖(Bahdanau 等， 2014;Vaswani 等，2017)。最近，Al-Rfou 等人(2018)设计了一套辅助损失，用于训练字符级语言建模的深度 Transformer 网络，其性能大大优于 LSTMs。尽管取得了成功，Al-Rfou 等人（2018）的 LM 训练是把几百个字符分隔成固定长度的片段上进行的，没有任何跨段的信息流。由于固定的上下文长度，模型不能捕获任何超出预定义上下文长度的长期依赖关系。此外，通过选择连续的符号块来创建固定长度的段，而不考虑句子或任何其他语义边界。因此，该模型缺乏必要的上下文信息来很好地预测前几个符号（应该指的是每段的前几个符号），导致优化不足和性能低下。我们将这个问题称为上下文碎片（context fragmentation）。

为了解决前面提到的固定长度上下文的限制，我们提出了一个名为Transformer-XL(意为超长)的新架构。我们将递归的概念引入我们深层 self-attention 网络。特别是，我们重用了前面片段获得的隐藏状态，而不是从头计算每个新片段的隐藏状态。被重用的隐藏状态充当当前片段的记忆，这在片段之间构建了循环连接。因此，建模非常长期的依赖关系成为可能，因为信息可以通过循环连接传播。同时，之前片段的信息也能解决上下文碎片化问题。更重要的是，我们展示了使用相对位置编码而不是绝对位置编码的必要性，以便在不造成时序混乱的情况下实现状态重用。因此，作为一项额外的技术贡献，我们引入了一个简单但更有效的相对位置编码公式，该公式能推广到比训练时看到的更长的注意力长度。

Transformer-XL在五个数据集上获得了很好的结果，这些数据集从单词级到字符级各不相同。在仅 100M tokens 上训练的 Transformer-XL 还能够生成具有数千个 tokens 的相对连贯的长文本文章。

我们的主要技术贡献包括在一个纯自注意力模型中引入递归的概念，并推导出一种新的位置编码方案。这两种技术构成了一组完整的解决方案，因为其中任何单独一种都不能解决固定长度上下文的问题。Transform-XL 是第一个在字符级和单词级语言建模方面都比 RNNs 取得更好效果的自注意力模型。

### 2 相关工作

在过去的几年里，语言建模领域见证了许多重大的进步，包括但不限于设计新的架构来更好地编码上下文(Bengio 等， 2003;Mikolov 等，2010;Merity 等，2016;Al-Rfou 等，2018)，改进正则化和优化算法(Gal 和 Ghahramani,2016)，加快 Softmax 计算(Grave 等，2016)，丰富输出分布家族(Yang 等，2017)。

为了在语言建模中捕获远程上下文，一直以来的方式是直接将更广泛上下文的表征作为附加输入输入到网络中。目前的工作从手动定义上下文表征到依赖从数据中学到的文档级主题。

更广泛地说，在通用序列建模中，如何捕获长期依赖性一直是一个长期存在的研究问题。从这个角度来看，自 LSTM 的普适性以来，人们一直致力于缓解梯度消失问题,包括更好的初始化(Le 等, 2015),额外损失信号(Trinh 等, 2018)，增强记忆结构（Ke 等，2018）和其他修改RNNs内部结构以更好优化。与之不同的是，我们的工作是基于Transformer的体系结构，并表明作为现实世界的任务的语言建模能够从学习长期依赖关系中获益。

### 3 模型

给出一个 tokens 为 $\mathrm{x}=(x_1,\cdots,x_T)$ 的语料，语言模型的任务是估计联合概率 $P(\mathrm{x})$,通常被自回归因子分解为 $P(\mathrm{x})=\prod_tP(x_t|\mathrm{x}*{<t})$。通过因子分解，问题简化为估计每个条件因子。在这项工作中，我们坚持使用标准的神经方法来建模条件概率。具体来说，使用一个可训练的神经网络将上下文$\mathrm{x}*{<t}$编码成一个固定大小的隐藏状态，并与词嵌入相乘得到 logits,然后将 logits 输入 Softmax 函数，生成关于下个 token 的类别概率分布

#### 3.1 普通的Transformer模型

为了将 Transformer 或 self-attention 应用到语言建模中，核心问题是如何训练 Transformer 将任意长的上下文有效地编码为固定大小的表征。假定给无限内存和计算量，一个简单的解决方案就是使用绝对的 Transformer 解码器(类似于前馈神经网络)处理整个上下文序列。然而，在实践中，由于资源有限，这通常是不可行的。